{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference:\n",
        "\n",
        "Ozeren, E. (n.d.). Word2Vec from scratch with Python. Medium.\n",
        "https://medium.com/@enozeren/word2vec-from-scratch-with-python-1bba88d9f221\n",
        "\n",
        "Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of\n",
        "word representations in vector space. arXiv preprint arXiv:1301.3781."
      ],
      "metadata": {
        "id": "_2HP_WOtqf37"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZFUTf9cg_Hx",
        "outputId": "8e4f9885-2fa6-4f5f-d2e8-d3fbddc26cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step1 : Generate corpus\n",
        "# We define 20 words in ourcorpus\n",
        "dog_cat_words = ['dog', 'cat', 'pet', 'house', 'animal', 'sleep', 'play']\n",
        "family_words = ['girl', 'boy', 'father', 'mother', 'family', 'house', 'marriage']\n",
        "king_queen_words = ['crown', 'queen', 'king', 'empire', 'country', 'rule', 'castle']\n",
        "\n",
        "#We will shuffle these words and generate random long sequences to create  out dataset\n",
        "dog_cat_text = ''\n",
        "family_text =''\n",
        "king_queen_text=''\n",
        "for _ in range (10000):\n",
        "  random.shuffle(dog_cat_words)\n",
        "  dog_cat_text = dog_cat_text + ' ' + ' '.join(dog_cat_words)\n",
        "  random.shuffle(family_words)\n",
        "  family_text = family_text + ' ' + ' '.join(family_words)\n",
        "  random.shuffle(king_queen_words)\n",
        "  king_queen_text = king_queen_text + ' ' + ' '.join(king_queen_words)\n",
        "\n",
        "\n",
        "\n",
        "small_corpus = dog_cat_text + family_text + king_queen_text"
      ],
      "metadata": {
        "id": "kzUwtUzkhH_z"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write corpus to file\n",
        "file_name = \"small_corpus.txt\"\n",
        "with open(file_name, 'w') as f:\n",
        "    f.write(small_corpus)\n",
        "# Read corpus from file\n",
        "file_path = \"small_corpus.txt\"\n",
        "with open(file_path, \"r\") as f:\n",
        "    text = f.read()\n"
      ],
      "metadata": {
        "id": "nWGCJn5phOVY"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== step 2： Generate a bunch of CBOW training pairs  ==========\n",
        "def generate_cbows(text, window_size):\n",
        "    text = text.lower()\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word.isalpha()]\n",
        "    unique_words = list(set(words))\n",
        "\n",
        "    cbows = []\n",
        "    for i in range(window_size, len(words) - window_size):\n",
        "        target_word = words[i]\n",
        "        context_words = []\n",
        "        for j in range(i - window_size, i + window_size + 1):\n",
        "            if j != i:\n",
        "                context_words.append(words[j])\n",
        "        cbows.append((context_words, target_word))\n",
        "\n",
        "    return cbows, unique_words\n",
        "\n",
        "window_size = 2\n",
        "cbows, unique_words = generate_cbows(text, window_size)"
      ],
      "metadata": {
        "id": "YGD5_Zc2h57I"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== step 3：One-Hot encoding ==========\n",
        "def one_hot_encoding(word, unique_words):\n",
        "    encoding = []\n",
        "    for w in unique_words:\n",
        "        if word == w:\n",
        "            encoding.append(1)\n",
        "        else:\n",
        "            encoding.append(0)\n",
        "    return torch.tensor(encoding, dtype=torch.float32)\n",
        "\n",
        "one_hot_encodings = {}\n",
        "for word in unique_words:\n",
        "    one_hot_encodings[word] = one_hot_encoding(word, unique_words)"
      ],
      "metadata": {
        "id": "Q2VEOdBsiey1"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== step4 converted to vector pair ==========\n",
        "cbow_vector_pairs = []\n",
        "for context_words, target_word in cbows:\n",
        "    context_vectors = [one_hot_encodings[word] for word in context_words]\n",
        "    target_vector = one_hot_encodings[target_word]\n",
        "    cbow_vector_pairs.append((context_vectors, target_vector))\n",
        "\n",
        "cbow_vector_pairs_summed = []\n",
        "for context_vectors, target_vector in cbow_vector_pairs:\n",
        "    context_sum = torch.stack(context_vectors).sum(dim=0)\n",
        "    cbow_vector_pairs_summed.append((context_sum, target_vector))"
      ],
      "metadata": {
        "id": "rt6i_6V9ilP1"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== step5：Create Dataset and DataLoader ==========\n",
        "class Word2VecDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "      self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return self.data[idx]\n",
        "\n",
        "split_idx = int(0.8 * len(cbow_vector_pairs_summed))\n",
        "train_data = cbow_vector_pairs_summed[:split_idx]\n",
        "val_data = cbow_vector_pairs_summed[split_idx:]\n",
        "\n",
        "train_dataset = Word2VecDataset(train_data)\n",
        "val_dataset = Word2VecDataset(val_data)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "validation_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "IC_Z4pnyinDQ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== step6 Define model ==========\n",
        "class NaiveWord2Vec(nn.Module):\n",
        "    def __init__(self, VOCAB_SIZE, VECTOR_DIM):\n",
        "        super().__init__()\n",
        "        self.vocab_size = VOCAB_SIZE\n",
        "        self.vector_dim = VECTOR_DIM\n",
        "        self.W1 = nn.Parameter(torch.randn(VOCAB_SIZE, VECTOR_DIM, requires_grad=True))\n",
        "        self.W2 = nn.Parameter(torch.randn(VECTOR_DIM, VOCAB_SIZE, requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = torch.matmul(x, self.W1)\n",
        "        output = torch.matmul(hidden, self.W2)\n",
        "        return output\n",
        "\n",
        "VOCAB_SIZE = len(unique_words)\n",
        "VECTOR_DIM = 10\n",
        "\n",
        "model = NaiveWord2Vec(VOCAB_SIZE, VECTOR_DIM)"
      ],
      "metadata": {
        "id": "aj2oCdBJk8Dx"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== step7 Define training function ==========\n",
        "def train_model(model, train_dataloader, validation_dataloader, epochs, learning_rate, verbose=False):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "    train_set_loss_log = []\n",
        "    validation_set_loss_log = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        if verbose:\n",
        "            print(f\"Epoch: {epoch + 1}/{epochs}\")\n",
        "\n",
        "        #Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        num_train_batches = 0\n",
        "\n",
        "        for inputs_batch, outputs_batch in train_dataloader:\n",
        "            y_train_logits = model(inputs_batch)\n",
        "\n",
        "\n",
        "            target_indices = torch.argmax(outputs_batch, dim=1)\n",
        "            train_loss = loss_fn(y_train_logits, target_indices)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += train_loss.item()\n",
        "            num_train_batches += 1\n",
        "\n",
        "        average_train_loss = total_train_loss / num_train_batches\n",
        "        train_set_loss_log.append(average_train_loss)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  Training loss: {average_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_validation_loss = 0.0\n",
        "        num_validation_batches = 0\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            for inputs_batch, outputs_batch in validation_dataloader:\n",
        "                y_val_logits = model(inputs_batch)\n",
        "\n",
        "\n",
        "                target_indices = torch.argmax(outputs_batch, dim=1)\n",
        "                validation_loss = loss_fn(y_val_logits, target_indices)\n",
        "\n",
        "                total_validation_loss += validation_loss.item()\n",
        "                num_validation_batches += 1\n",
        "\n",
        "        average_validation_loss = total_validation_loss / num_validation_batches\n",
        "        validation_set_loss_log.append(average_validation_loss)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  Validation loss: {average_validation_loss:.4f}\")\n",
        "\n",
        "    return model, train_set_loss_log, validation_set_loss_log"
      ],
      "metadata": {
        "id": "a5CelwPFk81Y"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== step8 Training model ==========\n",
        "model, train_losses, val_losses = train_model(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    validation_dataloader,\n",
        "    epochs=10,\n",
        "    learning_rate=0.001,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWzj1OLGk_r3",
        "outputId": "399d9c01-e2b9-430d-d291-31afcde988e9"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "  Training loss: 2.3932\n",
            "  Validation loss: 1.5851\n",
            "Epoch: 2/10\n",
            "  Training loss: 1.5704\n",
            "  Validation loss: 1.5689\n",
            "Epoch: 3/10\n",
            "  Training loss: 1.5671\n",
            "  Validation loss: 1.5719\n",
            "Epoch: 4/10\n",
            "  Training loss: 1.5664\n",
            "  Validation loss: 1.5719\n",
            "Epoch: 5/10\n",
            "  Training loss: 1.5659\n",
            "  Validation loss: 1.5681\n",
            "Epoch: 6/10\n",
            "  Training loss: 1.5660\n",
            "  Validation loss: 1.5663\n",
            "Epoch: 7/10\n",
            "  Training loss: 1.5658\n",
            "  Validation loss: 1.5707\n",
            "Epoch: 8/10\n",
            "  Training loss: 1.5657\n",
            "  Validation loss: 1.5713\n",
            "Epoch: 9/10\n",
            "  Training loss: 1.5658\n",
            "  Validation loss: 1.5682\n",
            "Epoch: 10/10\n",
            "  Training loss: 1.5656\n",
            "  Validation loss: 1.5676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== step9 Extract word vector ==========\n",
        "params = list(model.parameters())\n",
        "word_vectors = params[0].detach()\n",
        "word_dict = {word: vector for word, vector in zip(unique_words, word_vectors)}\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "print(f\"vocabulary size: {len(unique_words)}\")\n",
        "print(f\"word vector dimension: {VECTOR_DIM}\")\n",
        "\n",
        "#  Examples of word vector\n",
        "print(\"\\n word vector（first 5 dimension）:\")\n",
        "for i, word in enumerate(list(unique_words)[:5]):\n",
        "    print(f\"  {word}: {word_dict[word][:5].numpy()}\")\n",
        "\n",
        "# cosine similarity\n",
        "def cosine_similarity(v1, v2):\n",
        "    return torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHgOWRgtlBKT",
        "outputId": "7563fcb5-adf1-4bfc-bd7c-4f4ac671838a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed!\n",
            "vocabulary size: 20\n",
            "word vector dimension: 10\n",
            "\n",
            " word vector（first 5 dimension）:\n",
            "  mother: [ 1.0194747 -1.3262519 -0.9400389 -0.6814113 -1.1793137]\n",
            "  father: [ 0.05292026 -0.59184104 -1.726279   -1.2563657  -0.48930538]\n",
            "  girl: [-0.05198452  0.9495627  -0.33856043 -0.50958073  0.15935855]\n",
            "  dog: [-0.08309487 -0.73545176 -0.20419116 -1.3450007   0.20620972]\n",
            "  sleep: [ 1.6995863  -0.28693202  0.13986307  0.51986074 -0.24724486]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Step10: What the model learned in training ==========\n",
        "# Check similarity between words\n",
        "print(\"\\nWord Similarity Analysis:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Words from the same topic should be more similar\n",
        "if 'dog' in word_dict and 'cat' in word_dict:\n",
        "    sim = cosine_similarity(word_dict['dog'], word_dict['cat'])\n",
        "    print(f\"dog <-> cat (both animals): {sim:.4f}\")\n",
        "\n",
        "if 'king' in word_dict and 'queen' in word_dict:\n",
        "    sim = cosine_similarity(word_dict['king'], word_dict['queen'])\n",
        "    print(f\"king <-> queen (both royalty): {sim:.4f}\")\n",
        "\n",
        "if 'father' in word_dict and 'mother' in word_dict:\n",
        "    sim = cosine_similarity(word_dict['father'], word_dict['mother'])\n",
        "    print(f\"father <-> mother (both family): {sim:.4f}\")\n",
        "\n",
        "# Words from different topics should be less similar\n",
        "if 'dog' in word_dict and 'king' in word_dict:\n",
        "    sim = cosine_similarity(word_dict['dog'], word_dict['king'])\n",
        "    print(f\"dog <-> king (different topics): {sim:.4f}\")\n",
        "\n",
        "if 'cat' in word_dict and 'crown' in word_dict:\n",
        "    sim = cosine_similarity(word_dict['cat'], word_dict['crown'])\n",
        "    print(f\"cat <-> crown (different topics): {sim:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8FxKul8lK-l",
        "outputId": "c049a55b-ea89-4456-c5c6-f7c7a3f0bb2d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Similarity Analysis:\n",
            "==================================================\n",
            "dog <-> cat (both animals): 0.1536\n",
            "king <-> queen (both royalty): 0.5359\n",
            "father <-> mother (both family): 0.6979\n",
            "dog <-> king (different topics): -0.5061\n",
            "cat <-> crown (different topics): -0.4150\n"
          ]
        }
      ]
    }
  ]
}