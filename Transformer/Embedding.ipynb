{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sWeLaX649Nnk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_torch = torch.rand(10,10)\n",
        "print(random_torch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd54sEbS-WJB",
        "outputId": "77ed7c18-40b2-46e7-9a9e-1713922272a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8227, 0.9969, 0.5801, 0.8747, 0.4922, 0.1568, 0.5062, 0.0335, 0.4668,\n",
            "         0.3284],\n",
            "        [0.9858, 0.9145, 0.3752, 0.7371, 0.7427, 0.0892, 0.5806, 0.5842, 0.2669,\n",
            "         0.1770],\n",
            "        [0.7603, 0.5618, 0.3569, 0.0947, 0.1111, 0.7848, 0.7828, 0.4665, 0.7872,\n",
            "         0.0978],\n",
            "        [0.8144, 0.2523, 0.9379, 0.7197, 0.3154, 0.9074, 0.1290, 0.1842, 0.1314,\n",
            "         0.8822],\n",
            "        [0.5495, 0.1723, 0.0069, 0.9086, 0.4995, 0.5573, 0.2646, 0.2925, 0.7011,\n",
            "         0.1124],\n",
            "        [0.7064, 0.0915, 0.3907, 0.9659, 0.0473, 0.6100, 0.5796, 0.2978, 0.6934,\n",
            "         0.6350],\n",
            "        [0.6954, 0.4064, 0.6177, 0.4183, 0.2656, 0.8991, 0.8846, 0.4114, 0.3415,\n",
            "         0.4522],\n",
            "        [0.2193, 0.6185, 0.1430, 0.3927, 0.5887, 0.3197, 0.0455, 0.3108, 0.2362,\n",
            "         0.1722],\n",
            "        [0.2563, 0.8129, 0.5054, 0.5048, 0.3997, 0.4723, 0.5634, 0.5536, 0.2803,\n",
            "         0.2889],\n",
            "        [0.9796, 0.3601, 0.0450, 0.2452, 0.8175, 0.8555, 0.9203, 0.6411, 0.8183,\n",
            "         0.0346]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#输入的词表索引转换为指定维度的embedding\n",
        "class TokenEmbedding(nn.Embedding):\n",
        "  def _init_(self, vocab_size, d_model):\n",
        "    super(TokenEmbedding, self)._init_(vocab_size, d_model, padding_idx=1)\n",
        ""
      ],
      "metadata": {
        "id": "-jyxH6wO_b_p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Position embedding\n",
        " #对于每一个词，生成sin and cos 位置编码\n",
        "class PositionalEmbedding (nn.Module):\n",
        "  def __init__(self,d_model, max_len, device ):\n",
        "    super().__init__(self, PositionalEmbedding)._init_()\n",
        "    self.encoding = torch.zero(max_len, d_model, device = device)\n",
        "    self.encoding.require_grad()=False\n",
        "    #变为张量\n",
        "    pos = torch.arange(0, max_len, device = device)\n",
        "    pos = pos.float()\n",
        "    pos = pos.unsqueeze(dim=1)\n",
        "    _zi = torch.arange(0, d_model, step=2, device = device).float()\n",
        "    self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_zi / d_model)))\n",
        "    self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_zi / d_model)))\n",
        "def forward(self,x):\n",
        "  batch_size,seq_len =x.size()\n",
        "  return self.encoding[:seq_len,:]\n",
        "\n"
      ],
      "metadata": {
        "id": "wzU-FTpj-ggU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
        "    super(TransformerEmbedding, self).__init__()\n",
        "    self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
        "    self.pos_emb =PositionalEmbedding(d_model, max_len, device)\n",
        "    self.drop_out = nn.Dropout(p=drop_prob) #防止过拟合\n",
        "  def forward(self, x):\n",
        "    tok_emb = self.tok_emb(x)\n",
        "    pos_emb = self.pos_emb(x)\n",
        "    return self.drop_out(tok_emb+ pos_emb)\n"
      ],
      "metadata": {
        "id": "1UcgmTAaGuIU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}